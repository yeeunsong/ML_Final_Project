{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import numpy.linalg as lin\n",
    "from numpy.linalg import LinAlgError\n",
    "from itertools import combinations_with_replacement, combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#쓰는 메소드만 만든다.\n",
    "class PolynomialFeatures:\n",
    "    '''\n",
    "    다항식으로 feature space 변환\n",
    "    쓰여진 모듈\n",
    "    1> itertools.combinations_with_replacement, combinations\n",
    "    2> numpy\n",
    "    '''\n",
    "    __slots__ = ['degree', 'interaction_only', 'include_bias', 'self_only']\n",
    "    def __init__(self, degree = 2, interaction_only = False, self_only = False, include_bias = True):\n",
    "        self.degree = degree\n",
    "        self.interaction_only = interaction_only\n",
    "        self.include_bias = include_bias\n",
    "        self.self_only = self_only\n",
    "        \n",
    "        if (self.self_only == True) and (self.interaction_only == True):\n",
    "            raise Exception(\"either self_only or interaction_only can be True\")\n",
    "        \n",
    "        if self.degree > 8: \n",
    "            raise Exception(\"Too high degree!!\")\n",
    "        \n",
    "            \n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        #1. 열 별로 추출\n",
    "        col_list = [col for col in X.T]\n",
    "        \n",
    "        #2. 열 별로 deg에 맞춰 새로운 X를 생성한다.\n",
    "        result_X = None\n",
    "        \n",
    "        for deg in range(1,self.degree+1): #self.degree까지 포함되어야 한다.\n",
    "            if deg == 1:\n",
    "                result_X = self.__combination(col_list, deg)\n",
    "            else:\n",
    "                result_X = np.vstack( (result_X, self.__combination(col_list, deg)) ) \n",
    "        \n",
    "       # model에서 bias를 따로 할당해준다.\n",
    "       # if self.include_bias == True:\n",
    "       #     result_X = np.vstack( (np.ones(col_list[0].shape),result_X) )\n",
    "        return result_X.T\n",
    "    \n",
    "    def __combination(self, col_list, n): #deg n 의 조합들 생성\n",
    "        \n",
    "        combination_cols = []\n",
    "        result = None\n",
    "        \n",
    "        if self.interaction_only == True:\n",
    "            combination_cols = combinations(col_list, n)\n",
    "        \n",
    "        elif self.self_only == True:\n",
    "            for col in col_list:\n",
    "                temp = [col]*n\n",
    "                combination_cols.append(temp)\n",
    "        \n",
    "        else:\n",
    "            combination_cols = combinations_with_replacement(col_list, n)\n",
    "        \n",
    "        \n",
    "        for idx, combination in enumerate(combination_cols):\n",
    "            current_col = np.full(col_list[0].shape, 1.0)\n",
    "            for col in combination:\n",
    "                current_col*=col\n",
    "        \n",
    "            if idx == 0:\n",
    "                result = current_col\n",
    "            else :\n",
    "                result = np.vstack((result, current_col))        \n",
    "        return result #return = ndarray type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. KNN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNRegressor:\n",
    "    def __init__(self, n=2, copy_train = True):\n",
    "        self.n = n\n",
    "        self.copy_train = copy_train\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.copy_train == True:\n",
    "            self.train_X = np.copy(X)\n",
    "            self.train_y = np.copy(y)\n",
    "        else:\n",
    "            self.train_X = X\n",
    "            self.train_y = y\n",
    "    \n",
    "    def predict(self, X):\n",
    "        distance = lambda x : (x).dot(x)\n",
    "        neighbors = [(distance(X - data), self.train_y[idx]) for idx, data in enumerate(self.train_X)]\n",
    "        #neighbors.sort() #거리순 정렬\n",
    "        selected = []\n",
    "        for _ in range(self.n):\n",
    "            min_idx = np.argmin(test, axis = 0)[0]\n",
    "            selected.append(neighbors[min_idx])\n",
    "            del neighbors[min_idx]\n",
    "        \n",
    "        neighbors = selected\n",
    "        \n",
    "        if self.n == 1:\n",
    "            return neighbors[0][1]\n",
    "        else:\n",
    "            neighbors = neighbors[:self.n]\n",
    "            #거리 구하기 + 예측하기\n",
    "            dist = np.array([d for d, _ in neighbors])\n",
    "            label = np.array([label for _, label in neighbors])\n",
    "        \n",
    "            prop = dist/np.sum(dist) #weighted average를 위해\n",
    "            predict_label = prop.dot(label)\n",
    "        \n",
    "            return predict_label\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = np.array([self.predict(data) for data in X])\n",
    "        \n",
    "        return self.__r2_score(y,y_pred)\n",
    "    \n",
    "    def __r2_score(self, y_true, y_pred):\n",
    "        SS_total = 0\n",
    "        SS_reg = 0\n",
    "    \n",
    "        for i in range(len(y_true)):\n",
    "            x = (y_true[i] - y_pred[i])**2\n",
    "            SS_reg = SS_reg+x\n",
    "    \n",
    "        y_true_mean = np.mean(y_true)\n",
    "        for i in range(len(y_true)):\n",
    "            x = (y_true[i]-y_true_mean)**2\n",
    "            SS_total = SS_total+x\n",
    "        return 1 - (SS_reg/SS_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from abc import *\n",
    "class base_linear_model(ABC):\n",
    "    def __init__(self, alpha = 0.01, copy_X = True, normalize = True, fit_intercept = True, max_iter = None):\n",
    "        self.alpha = alpha\n",
    "        self.copy_X = copy_X\n",
    "        self.normalize = normalize\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.max_iter = max_iter\n",
    "        self.train_mean = None\n",
    "        self.train_std = None\n",
    "        \n",
    "    @abstractmethod\n",
    "    def fit(self,X,y):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def score(self, X,y):\n",
    "        pass        \n",
    "    \n",
    "    @abstractmethod\n",
    "    def gradient_descent(self, X, y):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def gradient(self, X, y, w):\n",
    "        pass\n",
    "    def normalize(self, data):\n",
    "        data = (data - self.train_mean)/self.train_std\n",
    "        return data\n",
    "    \n",
    "    def r2_score(self, y_true, y_pred):\n",
    "        SS_total = 0\n",
    "        SS_reg = 0\n",
    "    \n",
    "        for i in range(len(y_true)):\n",
    "            x = (y_true[i] - y_pred[i])**2\n",
    "            SS_reg = SS_reg+x\n",
    "    \n",
    "        y_true_mean = np.mean(y_true)\n",
    "        for i in range(len(y_true)):\n",
    "            x = (y_true[i]-y_true_mean)**2\n",
    "            SS_total = SS_total+x\n",
    "        return 1 - (SS_reg/SS_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-1: Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Ridge(base_linear_model):\n",
    "    '''\n",
    "    alpha: 제약 정도\n",
    "    normalize: data를 정규화 할 것인가\n",
    "    max_iter: 경사하강법 사용시, 반복의 정도\n",
    "    '''\n",
    "    def __init__(self, alpha = 0.01, copy_X = True, normalize = False, max_iter = 1000):\n",
    "        super().__init__(alpha = alpha, copy_X = copy_X, normalize = normalize, max_iter = max_iter)\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.weight = None\n",
    "        \n",
    "        if self.copy_X == True:\n",
    "            self.X_train = np.copy(X)\n",
    "        else:\n",
    "            self.X_train = X\n",
    "        \n",
    "        if self.normalize == True:\n",
    "            self.train_mean = self.X_train.mean()\n",
    "            self.train_std = self.X_train.std()\n",
    "            \n",
    "            self.X_train = super().normalize(self.X_train)\n",
    "        \n",
    "        self.X_train = np.concatenate((self.X_train, np.ones((self.X_train.shape[0],1))), axis = 1)\n",
    "        \n",
    "        try :\n",
    "            XT = self.X_train.T\n",
    "            step1 = lin.inv(XT@self.X_train + self.alpha*(np.eye(XT.shape[0])))\n",
    "            self.weight = step1@XT@y\n",
    "        except LinAlgError as e: #inv를 구할 수 없다. ==> 경사하강법 사용\n",
    "            \n",
    "            self.weight = np.random.randn(self.X_train.shape[1]) #weight initialize\n",
    "            self.weight = self.gradient_descent(self.X_train,y) #경사하강법으로 찾는다.\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.normalize == True:\n",
    "            X = super().normalize(X)\n",
    "        \n",
    "        if len(X.shape) <= 1:\n",
    "            X = np.hstack((X, np.array([1])))\n",
    "        else:\n",
    "            X = np.concatenate((X, np.ones((X.shape[0],1))), axis = 1)\n",
    "        \n",
    "        return np.dot(X,self.weight)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        predict_y = self.predict(X)\n",
    "        score = super().r2_score(y, predict_y)\n",
    "        return score\n",
    "    \n",
    "    def gradient_descent(self, X,y):\n",
    "        #heavy ball method\n",
    "        #1. 초기값 정하기\n",
    "        tolerence = 1.0e-2\n",
    "        bethas = np.logspace(-2,0,3) #중력\n",
    "        steps = np.logspace(-8,0,11) #이동 거리\n",
    "        prv_weight = np.copy(self.weight) #중력 방향\n",
    "        \n",
    "        for _ in range(self.max_iter):\n",
    "            gradient = self.gradient(X,y, self.weight)\n",
    "            direction = (-1) * (gradient/np.sqrt(gradient.dot(gradient)))\n",
    "            \n",
    "            #2. 종료조건 탐색 ==> 기울기가 0에 수렴하는가.\n",
    "            if gradient.dot(gradient) < tolerence:\n",
    "                return self.weight\n",
    "            \n",
    "            #3. 다음 위치 찾기\n",
    "            next_weights = [(self.weight + step*direction + betha*(self.weight - prv_weight)) for step in steps for betha in bethas]\n",
    "            \n",
    "            next_gradients = [self.gradient(X,y,w) for w in next_weights]\n",
    "            \n",
    "            next_gradients = [g.dot(g) for g in next_gradients]\n",
    "            \n",
    "            min_idx = np.argmin(next_gradients)\n",
    "            \n",
    "            #4.update\n",
    "            prv_weight = np.copy(self.weight) #중력 방향\n",
    "            self.weight = next_weights[min_idx]\n",
    "                \n",
    "        return self.weight\n",
    "            \n",
    "    \n",
    "    def gradient(self, X, y, w):\n",
    "        X_T = X.T\n",
    "        #print('X_T@X: ',X_T@X)\n",
    "        #print('X_T@y: ', X_T@y)\n",
    "        return -2*( X_T@y - ( X_T@X + self.alpha*np.eye(len(w)) )@w )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#poly = PolynomialFeatures(degree = 5, interaction_only = False, self_only = False)\n",
    "#model = Ridge(normalize = True, alpha = 0.01, max_iter = 10000)\n",
    "model = KNNRegressor(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('~/ml/data/test3_data/test3_data_train.csv')\n",
    "test_data = pd.read_csv('~/ml/data/test3_data/test3_data_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['prop_log_historical_price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 점수 내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_price = train_data.drop(target, axis = 'columns').values\n",
    "train_target_price = train_data[target[0]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data_price = poly.fit_transform(train_data_price.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_data_price, train_target_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_price = test_data.drop(target, axis = 'columns').values\n",
    "test_target_price = test_data[target[0]].values\n",
    "#test_data_price = poly.fit_transform(test_data_price.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-44927dac68b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_price\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_target_price\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-f412603dc449>\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__r2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-f412603dc449>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__r2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-f412603dc449>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mselected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mmin_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mselected\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneighbors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmin_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmin_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "model.score(test_data_price, test_target_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<경사하강법>\n",
    "\n",
    "시도 1: 단순 경사하강법<step 1개> << w <= w + step * direction >>\n",
    "결과 1: optimal을 찾는데 실패했고, 0.32정도의 결과를 얻었다.\n",
    "기타 1:\n",
    "        > 기울기를 구하는데 있어, X가 정규화 되지 않으면, 적합한 해를 구할 수 없음을 알았다\n",
    "             > normalize를 하지 않았을 때, 약 1.0e-300정도의 r2-score를 얻었고, normalize를 했을 때, 0.32정도의 r2-score를 얻었다.\n",
    "        > 종료 조건을 w의 각 entry가 tolerance보다 작은지 비교하는 형태를 취하였다.\n",
    "             > 종료 조건의 문제인지 iteration을 도는 동안 optimal을 얻지 못한 것 같다.\n",
    "        > step을 키웠을 때, overflow error가 발생하였다.\n",
    "             > gradient의 크기가 매우 커져, weight 자체를 구할 수 없는 상황이 있었다\n",
    "        > max_iter를 증가해도, optimal을 찾지 못하는 것으로 보아, 구현 자체에 문제가 있다고 판단된다.\n",
    "             \n",
    "시도 2: 단순 경사하강법<step 여러개>\n",
    "결과 2: optimal을 찾는데 실패했고, 시도 1보다 약간 더 좋은 결과를 얻었지만, 여전히 optimal 값을 얻지 못했다.\n",
    "기타 2: \n",
    "        > step을 여러 개 두어, 다음 weight의 위치를 더 좋게 만들고자 했으며, 이를 위해 복잡도를 약간 희생했다.(11정도의 상수가 곱해짐)\n",
    "        > overflow error가 발생하는 것을 해결하지 못했다.\n",
    "        > 최저점을 찾는데 있어, 종료 조건으로 vector size < tolerance<약 1.0e-4>라고 해야 함을 알았다\n",
    "            >이 때, g의 내적을 한 뒤, 제곱근을 취하는 대신, 그냥 tolerance를 1.0e-2로 두어 계산량을 줄였다.\n",
    "        > max_iter를 증가해도, optimal을 찾지 못하는 것으로 보아, 구현 자체에 문제가 있다고 판단된다.\n",
    "        \n",
    "시도 3: heavy ball method<step여러개, betha 한개> << w <= w + step * direction + betha * (w - prv_w) >>\n",
    "결과 3: optimal을 찾는데 실패했고, 시도 2보다 약간 더 좋은 결과를 얻었다.\n",
    "기타 3: \n",
    "        > direction을 찾는데 있어 치명적인 실수가 있었음을 발견했다.\n",
    "              > 단순히 gradient를 방향으로 치환하여 사용한 오류를 발견했고, 이후, 단위벡터로 치환하여 이용하였다.\n",
    "              > overflow error가 사라졌다.\n",
    "        > max_iter를 증가하면, r2 score가 올라가긴 하지만, 시간이 너무 오래 걸리는 단점이 있다.\n",
    "              > step의 크기가 부정확한 문제가 있는 것으로 판단된다.\n",
    "\n",
    "시도 4: heavy ball method<<step 여러개, betha 여러개>>\n",
    "결과 4: optimal을 찾는데 성공했다.\n",
    "기타 4: \n",
    "        > betha와 step을 여러개 두어, 다음의 최적 지점을 보다 잘 찾을 수 있도록 만들었다.\n",
    "        > initial weight와 optimal weight를 보았을 때, 큰 크기의 수 변화가 일어나지 않아 step의 max를 1로 두었다.\n",
    "        > max_iter를 전부 순회하기 전, optimal을 찾을 수 있었다.\n",
    "\n",
    "<최종 분석>\n",
    "1. 단순 경사하강법 보다, heavy ball method가 더 좋은 결과를 도출했다.\n",
    "2. 실제 구현에 있어 데이터의 전처리<normalize>가 중요함을 알게 되었다.\n",
    "3. 방향을 찾는데 있어, 단위 벡터로 해야 하는 이유를 알게 되었다<overflow error가 발생하고, 단순 gradient는 해당 방향으로 매번 일정하게 이동함을 보장하지 않는다>\n",
    "4. ridge regression의 경우 loss function이 convex임을 미리 알고 있었기 때문에, FONC조건과 cvx의 성질을 이용하여 단순히 gradient size == 0일 때, optimal임을 받아들였다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
